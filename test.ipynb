{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([1, 1, 28, 28])\n",
      "Shape of y: torch.Size([1]) torch.int64\n",
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "BP_X_grad norm: tensor(4.5050e-07, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "ZO_X_grad norm: tensor(4.5833e-07, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "BP_X_grad-ZO_X_grad error norm: tensor(1.5386e-07, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"/home/yequan/Project/TensorFusionOperatorLearning/data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"/home/yequan/Project/TensorFusionOperatorLearning/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256, bias=True)\n",
    "        self.nl1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 256, bias=True)\n",
    "        self.nl2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.nl1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.nl2(x)\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"./temp/model.pth\"))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=1e-3)\n",
    "\n",
    "sigma = 1\n",
    "STP = True\n",
    "\n",
    "def get_gradient_vec(model):\n",
    "    gradients_list = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients_list.append(param.grad.view(-1))\n",
    "        else:\n",
    "            param.requires_grad_(False)\n",
    "    gradients_vec = torch.cat(gradients_list)\n",
    "\n",
    "    return gradients_vec \n",
    "\n",
    "def split_model(model):\n",
    "    modules = []\n",
    "    for m in model.children():\n",
    "        if isinstance(m, (torch.nn.Sequential,)):\n",
    "            modules += split_model(m)\n",
    "        else:\n",
    "            modules.append(m)\n",
    "    return modules\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    X, y = next(iter(dataloader))\n",
    "    for batch in range(1):\n",
    "    # for batch, (X, y) in enumerate(dataloader):\n",
    "    \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X.requires_grad_(True)\n",
    "\n",
    "        X = torch.flatten(X,1)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        BP_X_grad = torch.autograd.grad(loss, X, create_graph=True)[0] # before loss.backward()\n",
    "        loss.backward()\n",
    "        FO_grad = get_gradient_vec(model)\n",
    "\n",
    "        # ZO-CGE to get input gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X_grad = torch.zeros_like(X, device=device)\n",
    "\n",
    "        for idx in range(X.shape[-1]):\n",
    "            tangent = torch.zeros(X.shape, device=device)\n",
    "            tangent[0][idx] = 1.0\n",
    "            with fwAD.dual_level():\n",
    "                dual_input = fwAD.make_dual(X, tangent)\n",
    "                assert fwAD.unpack_dual(dual_input).tangent is tangent\n",
    "                \n",
    "                pred_dual = model(dual_input)\n",
    "                loss_dual = loss_fn(pred_dual, y)\n",
    "\n",
    "                # Unpacking the dual returns a ``namedtuple`` with ``primal`` and ``tangent`` as attributes\n",
    "                X_grad[0][idx] = fwAD.unpack_dual(loss_dual).tangent\n",
    "        \n",
    "        # for idx in range(X.shape[-1]):\n",
    "        #     old_value = X.data[0][idx] * 1.0\n",
    "\n",
    "        #     pos_perturbed_value = old_value + sigma\n",
    "        #     neg_perturbed_value = old_value - sigma\n",
    "\n",
    "        #     with torch.no_grad():  # training=True to enable profiling, but do not save graph\n",
    "        #         X.data[0][idx] = pos_perturbed_value  \n",
    "        #         # print(X.data[0][idx])    \n",
    "        #         temp_y = model(X)\n",
    "        #         pos_loss = loss_fn(pred, y)\n",
    "\n",
    "        #         if STP == True:\n",
    "        #             X.data[0][idx] = neg_perturbed_value\n",
    "        #             # print(X.data[0][idx])\n",
    "        #             temp_y = model(X)\n",
    "        #             neg_loss = loss_fn(pred, y)\n",
    "\n",
    "        #             X_grad[0][idx] = (pos_loss-neg_loss)/2/sigma\n",
    "        #         else:\n",
    "        #             X_grad[0][idx] = (pos_loss-loss)/sigma\n",
    "                \n",
    "        #         X.data[0][idx] = old_value\n",
    "        \n",
    "        print('BP_X_grad norm:', torch.linalg.norm(BP_X_grad.view(-1)))\n",
    "        print('ZO_X_grad norm:', torch.linalg.norm(X_grad.view(-1)))\n",
    "        print('BP_X_grad-ZO_X_grad error norm:', torch.linalg.norm(BP_X_grad.view(-1)-X_grad.view(-1)))\n",
    "        \n",
    "        # Forward chains rule\n",
    "        y1 = model.nl1(model.fc1(X))\n",
    "        mask = torch.sign(y1)\n",
    "        b1_grad = F.linear(X_grad, model.fc1.weight, None)\n",
    "        y1_grad = F.linear(X_grad, model.fc1.weight, None) * mask\n",
    "        # y1_grad = model.fc1(X_grad) * mask\n",
    "\n",
    "        W1_grad = torch.outer(y1_grad.squeeze(), X.squeeze())\n",
    "        model.fc1.weight.grad = W1_grad\n",
    "        model.fc1.bias.grad = b1_grad.squeeze()\n",
    "\n",
    "        y2 = model.nl2(model.fc2(y1))\n",
    "        mask = torch.sign(y2)\n",
    "        b2_grad = F.linear(y1_grad, model.fc2.weight, None)\n",
    "        y2_grad = F.linear(y1_grad, model.fc2.weight, None) * mask\n",
    "\n",
    "        W2_grad = torch.outer(y2_grad.squeeze(), y1.squeeze())\n",
    "        model.fc2.weight.grad = W2_grad\n",
    "        model.fc2.bias.grad = b2_grad.squeeze()\n",
    "\n",
    "        y3 = model.fc3(y2)\n",
    "        b3_grad = F.linear(y2_grad, model.fc3.weight, None)\n",
    "        y3_grad = F.linear(y2_grad, model.fc3.weight, None)\n",
    "\n",
    "        W3_grad = torch.outer(y3_grad.squeeze(), y2.squeeze())\n",
    "        model.fc3.weight.grad = W3_grad\n",
    "        model.fc3.bias.grad = b3_grad.squeeze()\n",
    "\n",
    "        ZO_grad = get_gradient_vec(model)\n",
    "\n",
    "        print('FO_grad norm:', torch.linalg.norm(FO_grad))\n",
    "        print('ZO_grad norm:', torch.linalg.norm(ZO_grad))\n",
    "        print('FO_grad-ZO_grad error norm:', torch.linalg.norm(FO_grad-ZO_grad))\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = torch.flatten(X,1)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    # test(test_dataloader, model, loss_fn)\n",
    "    torch.save(model.state_dict(), \"./temp/model.pth\")\n",
    "    print(\"Saved PyTorch Model State to model.pth\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "\n",
    "primal = torch.arange(1,11,dtype=torch.float32)\n",
    "tangent = torch.zeros(10)\n",
    "tangent[0] = 1\n",
    "\n",
    "def fn(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "# All forward AD computation must be performed in the context of\n",
    "# a ``dual_level`` context. All dual tensors created in such a context\n",
    "# will have their tangents destroyed upon exit. This is to ensure that\n",
    "# if the output or intermediate results of this computation are reused\n",
    "# in a future forward AD computation, their tangents (which are associated\n",
    "# with this computation) won't be confused with tangents from the later\n",
    "# computation.\n",
    "with fwAD.dual_level():\n",
    "    # To create a dual tensor we associate a tensor, which we call the\n",
    "    # primal with another tensor of the same size, which we call the tangent.\n",
    "    # If the layout of the tangent is different from that of the primal,\n",
    "    # The values of the tangent are copied into a new tensor with the same\n",
    "    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n",
    "    #\n",
    "    # It is also important to note that the dual tensor created by\n",
    "    # ``make_dual`` is a view of the primal.\n",
    "    dual_input = fwAD.make_dual(primal, tangent)\n",
    "    assert fwAD.unpack_dual(dual_input).tangent is tangent\n",
    "\n",
    "    # Tensors that do not have an associated tangent are automatically\n",
    "    # considered to have a zero-filled tangent of the same shape.\n",
    "    plain_tensor = torch.arange(1,11,dtype=torch.float32)\n",
    "    dual_output = fn(dual_input, plain_tensor)\n",
    "\n",
    "    # Unpacking the dual returns a ``namedtuple`` with ``primal`` and ``tangent``\n",
    "    # as attributes\n",
    "    jvp = fwAD.unpack_dual(dual_output).tangent\n",
    "    print(jvp)\n",
    "\n",
    "assert fwAD.unpack_dual(dual_output).tangent is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "torch.Size([6, 1])\n",
      "(2, 3)\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([[-1, 0, 1], [2, 3, 4]])\n",
    "\n",
    "A[A<0] = 0\n",
    "\n",
    "print(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcunetv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
