resume: 0

data_provider:
  root: '~/dataset'

  # dataset: flowers
  # num_classes: 102
  # load_model_path: '/home/yequan/Project/tiny-training/algorithm/runs/flowers102/mcunet-5fps/FO/20240331-001718-1312245/checkpoint/ckpt.best.pth'
  
  dataset: cars
  num_classes: 196

  # dataset: aircraft
  # num_classes: 100
  
  # dataset: food101
  # num_classes: 101
  
  # dataset: cub200
  # num_classes: 200
  
  # dataset: pets
  # num_classes: 37

  # dataset: cifar10
  # num_classes: 10

  # base_batch_size: 1
  base_batch_size: 128

run_config:
  # warmup_epochs: 5
  # n_epochs: 100
  n_epochs: 4000
  bs256_lr: 0.1
  # bs256_lr: 0.01

  # iteration_decay: 0

  # optimizer
  # optimizer_name: sgd
  # optimizer_name: sgd_nomom
  # optimizer_name: adam

  # optimizer_name: sgd_int
  # optimizer_name: sgd_int_nomom

  # optimizer_name: sgd_scale
  # optimizer_name: sgd_scale_nomom

  # optimizer_name: sgd_scale_int
  optimizer_name: sgd_scale_int_nomom

net_config:
  net_name: mcunet-5fps
  # net_name: mbv2-w0.35
  # net_name: proxyless-w0.3

backward_config:
  # quantize_gradient: 1

  # sparse update (100KB scheme): 88.84%
  # enable_backward_config: 1  
  # n_bias_update: 22
  # manual_weight_idx: 21-24-27-30-36-39
  # # weight_update_ratio: 1-1-1-1-0.125-0.25
  # weight_update_ratio: 1-1-1-1-1-1

  # update last 2 blocks
  # enable_backward_config: 1 
  # n_bias_update: 6
  # n_weight_update: 6

  # update last 12 weights and biases (best setting for last k layers): 87.74%, uses 448KB
  # enable_backward_config: 1 
  # n_bias_update: 12
  # n_weight_update: 12

  # bias-only
  # enable_backward_config: 1 
  # n_bias_update: 22
  # n_weight_update: 0

  # enable_backward_config: 1
  # n_bias_update: 0
  # n_weight_update: 0

  enable_backward_config: 0

train_config:
  # grad_output_prune_ratio: 0.9

  # layerwise_update: 'all'
  # layerwise_update: 'one'

  # layerwise_update_layer_list: ['1.7.conv.0', '1.8.conv.0', '1.9.conv.0', '1.10.conv.0', '1.12.conv.0', '1.13.conv.0']
  # layerwise_update_layer_list: ['1.6.conv.0', '1.7.conv.0', '1.8.conv.0', '1.9.conv.0', '1.10.conv.0', '1.11.conv.0', '1.12.conv.0', '1.13.conv.0']

  # train_scale: 1
  # train_normalization: 1

  # normalization_func: BN
  # normalization_func: GN
  # normalization_func: SSF

  # normalization_func: L1FRN
  # activation_func: TLU

  q_param_lr:  {
    'scale_y': 0.01,
    'scale_w': 0.01, 

    'gamma': 0.1,
    'beta': 0.1,
  }
  
ZO_Estim:
  en: True

  # fc_bp: False
  # fc_bp: cls_only
  fc_bp: 'partial_BP'
  # fc_bp: 'break_BP'

  name: ZO_Estim_MC
  n_sample: 10
  signSGD: False

  # trainable_param_list: all
  # trainable_param_list: ['weight', 'bias']
  trainable_param_list: ['weight']
  # trainable_param_list: ['bias', 'scale_y','zero_y','scale_w', 'gamma', 'beta']
  # trainable_param_list: ['1.7.conv.0.weight', '1.8.conv.0.weight', '1.9.conv.0.weight', '1.10.conv.0.weight', '1.12.conv.0.weight', '1.13.conv.0.weight']
  # trainable_param_list: ['1.7.conv.0.weight', '1.8.conv.0.weight', '1.9.conv.0.weight', '1.10.conv.0.weight', '1.12.conv.0.weight', '1.13.conv.0.weight', '1.10.conv.1.bias', '1.10.conv.2.bias', '1.11.conv.0.bias', '1.11.conv.1.bias', '1.11.conv.2.bias', '1.12.conv.0.bias', '1.12.conv.1.bias', '1.12.conv.2.bias', '1.13.conv.0.bias', '1.13.conv.1.bias', '1.13.conv.2.bias']
  
  # trainable_layer_list: block-all
  # trainable_layer_list: layer-all
  # trainable_layer_list: layer-first-20
  # trainable_layer_list: layer-last-30

  # trainable_layer_list: ['1.8']
  # trainable_layer_list: ['1.12', '1.13']
  # trainable_layer_list: ['1.7', '1.8', '1.9', '1.10', '1.11', '1.12', '1.13']
  trainable_layer_list: ['1.9.conv.0']
  # trainable_layer_list: ['1.7.conv.0', '1.8.conv.0', '1.9.conv.0', '1.10.conv.0']
  # trainable_layer_list: ['1.7.conv.0', '1.8.conv.0', '1.9.conv.0', '1.10.conv.0', '1.12.conv.0', '1.13.conv.0']
  # trainable_layer_list: ['1.6.conv.0', '1.7.conv.0', '1.8.conv.0', '1.9.conv.0', '1.10.conv.0', '1.11.conv.0', '1.12.conv.0', '1.13.conv.0']
  # trainable_layer_list: ['1.7.conv.2', '1.8.conv.2', '1.9.conv.2', '1.10.conv.2', '1.12.conv.2', '1.13.conv.2']
  # trainable_layer_list: ['1.12.conv.0', '1.12.conv.1', '1.12.conv.2', '1.13.conv.0', '1.13.conv.1', '1.13.conv.2']

  # estimate_method: forward
  estimate_method: antithetic

  # sample_method: uniform
  # sample_method: gaussian
  # sample_method: bernoulli
  sample_method: coord_basis

  ############################## Activation Perturbation ##############################
  # obj_fn_type: classifier_layerwise
  # perturb_method: activation

  # sigma: 10

  # perturb_before_round: 0

  # sync_batch_perturb: 0
  # # sync_batch_perturb: 1

  ############################## Param Perturbation ##############################
  obj_fn_type: classifier_layerwise
  perturb_method: param

  sigma: {
    'weight': 100,
    'bias': 1000,

    # 'scale_y': 0.01,
    # 'scale_w': 0.001,

    # 'gamma': 0.1,
    # 'beta': 0.1,
  }

  param_update_method: None
  # param_update_method: layerwise
  # param_update_method: blockwise
  # param_update_method: modelwise

  ############################## Weight Perturbation ##############################
  # obj_fn_type: classifier
  # perturb_method: batch
  
  # obj_fn_type: classifier_row
  # perturb_method: single

  # quantize_method: None
  # sigma: 0.01
  # quantize_method: u_fp-grad_fp
  # quantize_method: u_fp-grad_int

  # sigma: 1
  quantize_method: u_int-grad_fp
  # quantize_method: u_int-grad_int

  mask_method: None
  # mask_method: block-30
  # mask_method: layer

  prior_method: None
  # prior_method: last_grad_perturb
  # prior_method: mov_avg_perturb
  # prior_method: last_grad_neighbor