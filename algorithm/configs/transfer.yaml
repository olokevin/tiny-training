resume: 0

data_provider:
  # root: /home/yequan/dataset/flowers102
  # num_classes: 102
  # load_model_path: '/home/yequan/Project/tiny-training/algorithm/runs/flowers102/mcunet-5fps/FO/20240331-001718-1312245/checkpoint/ckpt.best.pth'
  
  root: /home/yequan/dataset/stanford_car
  num_classes: 196
  # Last only
  load_model_path: '/home/yequan/Project/tiny-training/algorithm/runs/stanford_car/mcunet-5fps/FO/20240418-184445-2021615/checkpoint/ckpt.best.pth'
  # 7.0
  # load_model_path: '/home/yequan/Project/tiny-training/algorithm/runs/stanford_car/mcunet-5fps/FO/20240417-001040-1964541/checkpoint/ckpt.best.pth'
  # 12+13
  # load_model_path: '/home/yequan/Project/tiny-training/algorithm/runs/stanford_car/mcunet-5fps/FO/20240417-000924-1962394/checkpoint/ckpt.best.pth'
  # only train last 2 blocks
  # load_model_path: '/home/yequan/Project/tiny-training/algorithm/runs/stanford_car/mcunet-5fps/FO/20240409-044117-1662606/checkpoint/ckpt.best.pth'
  
  # root: /home/yequan/dataset/aircraft
  # num_classes: 100
  
  # root: /home/yequan/dataset/food101
  # num_classes: 101
  
  # root: /home/yequan/dataset/cub200
  # num_classes: 200
  
  # root: /home/yequan/dataset/pets
  # num_classes: 37

  # dataset: cifar10
  # num_classes: 10

  base_batch_size: 64

run_config:
  warmup_epochs: 0
  # n_epochs: 3
  bs256_lr: 0.01

  iteration_decay: 0

  # optimizer
  # optimizer_name: sgd
  # optimizer_name: adam

  # optimizer_name: sgd_int
  # optimizer_name: sgd_int_nomom

  # optimizer_name: sgd_scale
  # optimizer_name: sgd_scale_nomom

  # optimizer_name: sgd_scale_int
  optimizer_name: sgd_scale_int_nomom

net_config:
  net_name: mcunet-5fps

backward_config:
  # quantize_gradient: 1

  # sparse update (100KB scheme): 88.84%
  # enable_backward_config: 1  
  # n_bias_update: 22
  # manual_weight_idx: 21-24-27-30-36-39
  # weight_update_ratio: 1-1-1-1-0.125-0.25

  # sparse update 
  # enable_backward_config: 1  
  # n_bias_update: 22
  # manual_weight_idx: 21
  # weight_update_ratio: 1
  # only_update_selected_bias: 1

  # update last 2 blocks (best setting for last k layers): 87.74%, uses 448KB
  # enable_backward_config: 1 
  # n_bias_update: 6
  # n_weight_update: 6

  # update last 12 weights and biases (best setting for last k layers): 87.74%, uses 448KB
  # enable_backward_config: 1 
  # n_bias_update: 22
  # n_weight_update: 0

  # enable_backward_config: 1
  # n_bias_update: 0
  # n_weight_update: 0

  enable_backward_config: 0

ZO_Estim:
  # en: True
  # fc_bp: False
  fc_bp: 'partial_BP'
  # fc_bp: 'break_BP'

  name: ZO_Estim_MC
  # sigma: 1
  n_sample: 1
  signSGD: False

  # trainable_param_list: all
  trainable_param_list: ['bias']
  # trainable_param_list: ['bias', 'effective_scale','zero_x','zero_y']
  # trainable_param_list: ['1.7.conv.0.weight', '1.8.conv.0.weight', '1.9.conv.0.weight', '1.10.conv.0.weight', '1.12.conv.0.weight', '1.13.conv.0.weight']
  # trainable_param_list: ['1.7.conv.0.weight', '1.8.conv.0.weight', '1.9.conv.0.weight', '1.10.conv.0.weight', '1.12.conv.0.weight', '1.13.conv.0.weight', '1.10.conv.1.bias', '1.10.conv.2.bias', '1.11.conv.0.bias', '1.11.conv.1.bias', '1.11.conv.2.bias', '1.12.conv.0.bias', '1.12.conv.1.bias', '1.12.conv.2.bias', '1.13.conv.0.bias', '1.13.conv.1.bias', '1.13.conv.2.bias']
  
  # trainable_layer_list: all
  trainable_layer_list: first-20
  # trainable_layer_list: last-30

  # trainable_layer_list: ['1.8']
  # trainable_layer_list: ['1.3', '1.6']
  # trainable_layer_list: ['1.8.conv.0']
  # trainable_layer_list: ['1.7.conv.0', '1.12.conv.0', '1.13.conv.0']
  # trainable_layer_list: ['1.7.conv.0', '1.8.conv.0', '1.9.conv.0', '1.10.conv.0', '1.12.conv.0', '1.13.conv.0']
  # trainable_layer_list: ['1.12.conv.0', '1.12.conv.1', '1.12.conv.2', '1.13.conv.0', '1.13.conv.1', '1.13.conv.2']
  
  # quantize_method: None
  # sigma: 0.01
  # quantize_method: u_fp-grad_fp
  # quantize_method: u_fp-grad_int

  # sigma: 10
  sigma: {
    'weight': 100,
    'bias': 100,
    'effective_scale': 0.001,
    'zero_x': 10,
    'zero_y': 10,
  }

  quantize_method: u_int-grad_fp
  # quantize_method: u_int-grad_int

  mask_method: None
  # mask_method: block-30
  # mask_method: layer

  # estimate_method: forward
  estimate_method: antithetic

  # obj_fn_type: classifier
  # perturb_method: batch

  # obj_fn_type: classifier_layerwise
  # perturb_method: activation

  obj_fn_type: classifier_layerwise
  perturb_method: param

  # obj_fn_type: classifier_row
  # perturb_method: single

  # sample_method: uniform
  # sample_method: gaussian
  # sample_method: bernoulli
  sample_method: coord_basis

  prior_method: None
  # prior_method: last_grad_perturb
  # prior_method: mov_avg_perturb
  # prior_method: last_grad_neighbor
  